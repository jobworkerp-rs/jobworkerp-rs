use crate::config::McpServerConfig;
use app::app::function::function_set::{FunctionSetApp, FunctionSetAppImpl};
use app::app::function::{FunctionApp, FunctionAppImpl};
use app_wrapper::llm::chat::conversion::ToolConverter;
use futures::StreamExt;
use rmcp::{
    ErrorData as McpError, RoleServer, ServerHandler, model::*, service::NotificationContext,
    service::RequestContext,
};
use std::collections::HashMap;
use std::sync::Arc;

/// MCP Server Handler that bridges jobworkerp's FunctionApp to MCP protocol.
///
/// Implements rmcp::ServerHandler to expose jobworkerp runners and workers
/// as MCP tools, enabling integration with MCP clients like Claude Desktop.
#[derive(Clone)]
pub struct McpHandler {
    function_app: Arc<FunctionAppImpl>,
    function_set_app: Arc<FunctionSetAppImpl>,
    config: McpServerConfig,
}

impl McpHandler {
    /// Create a new McpHandler with the given app modules and configuration
    pub fn new(
        function_app: Arc<FunctionAppImpl>,
        function_set_app: Arc<FunctionSetAppImpl>,
        config: McpServerConfig,
    ) -> Self {
        Self {
            function_app,
            function_set_app,
            config,
        }
    }

    /// Map internal errors to MCP ErrorData
    fn map_error(e: anyhow::Error) -> McpError {
        use jobworkerp_base::error::JobWorkerError;

        if let Some(jwe) = e.downcast_ref::<JobWorkerError>() {
            match jwe {
                JobWorkerError::NotFound(_) => {
                    McpError::method_not_found::<CallToolRequestMethod>()
                }
                JobWorkerError::InvalidParameter(msg) => {
                    McpError::invalid_params(msg.clone(), None)
                }
                JobWorkerError::WorkerNotFound(_) => {
                    McpError::method_not_found::<CallToolRequestMethod>()
                }
                _ => McpError::internal_error(e.to_string(), None),
            }
        } else {
            McpError::internal_error(e.to_string(), None)
        }
    }
}

impl ServerHandler for McpHandler {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            protocol_version: ProtocolVersion::LATEST,
            capabilities: ServerCapabilities::builder().enable_tools().build(),
            server_info: Implementation::from_build_env(),
            instructions: Some(
                "jobworkerp MCP Server - Asynchronous job processing with various runners. \
                 Available runners include COMMAND, HTTP_REQUEST, PYTHON_COMMAND, DOCKER, \
                 LLM_COMPLETION, INLINE_WORKFLOW, REUSABLE_WORKFLOW, and custom plugins."
                    .to_string(),
            ),
        }
    }

    async fn list_tools(
        &self,
        _request: Option<PaginatedRequestParams>,
        _context: RequestContext<RoleServer>,
    ) -> Result<ListToolsResult, McpError> {
        let functions = if let Some(name) = &self.config.set_name {
            self.function_set_app
                .find_functions_by_set(name)
                .await
                .map_err(Self::map_error)?
        } else {
            self.function_app
                .find_functions(
                    self.config.exclude_runner_as_tool,
                    self.config.exclude_worker_as_tool,
                )
                .await
                .map_err(Self::map_error)?
        };

        ToolConverter::convert_functions_to_mcp_tools(functions)
            .map_err(|e| McpError::internal_error(e.to_string(), None))
    }

    async fn call_tool(
        &self,
        request: CallToolRequestParams,
        _context: RequestContext<RoleServer>,
    ) -> Result<CallToolResult, McpError> {
        // Tool name is passed directly to call_function_for_llm().
        // The internal find_runner_by_name_with_mcp() handles the "runner___method" format
        // via divide_names(), so no pre-processing is needed here.
        let name = request.name.as_ref();

        // Arguments from MCP client follow the schema generated by ToolConverter:
        // { "settings": {...}, "arguments": {...} }
        // call_function_for_llm() internally uses prepare_runner_call_arguments()
        // which extracts these fields.
        let arguments = request.arguments;

        let meta = Arc::new(HashMap::new());

        if self.config.streaming {
            // Streaming execution: collect all results from the stream
            let stream = self.function_app.call_function_for_llm_streaming(
                meta,
                name,
                arguments,
                self.config.timeout_sec,
            );

            let mut collected_output = Vec::new();
            let mut last_error: Option<anyhow::Error> = None;
            let mut final_info = None;

            futures::pin_mut!(stream);
            while let Some(result) = stream.next().await {
                match result {
                    Ok(function_result) => {
                        // Collect non-empty output chunks
                        if !function_result.output.is_empty() {
                            collected_output.push(function_result.output);
                        }
                        // Preserve execution info from final chunk
                        if function_result.last_info.is_some() {
                            final_info = function_result.last_info;
                        }
                    }
                    Err(e) => {
                        last_error = Some(e);
                        break;
                    }
                }
            }

            if let Some(e) = last_error {
                return Err(Self::map_error(e));
            }

            // Combine all collected outputs
            let combined_output = if collected_output.is_empty() {
                serde_json::json!({
                    "status": "success",
                    "output": "",
                    "execution_info": final_info
                })
            } else {
                serde_json::json!({
                    "status": "success",
                    "output": collected_output.join(""),
                    "execution_info": final_info
                })
            };

            Ok(CallToolResult::success(vec![Content::json(
                combined_output,
            )?]))
        } else {
            // Non-streaming execution
            let result = self
                .function_app
                .call_function_for_llm(meta, name, arguments, self.config.timeout_sec)
                .await
                .map_err(Self::map_error)?;

            // Convert the result to MCP CallToolResult
            Ok(CallToolResult::success(vec![Content::json(result)?]))
        }
    }

    async fn initialize(
        &self,
        _request: InitializeRequestParams,
        context: RequestContext<RoleServer>,
    ) -> Result<InitializeResult, McpError> {
        // Log HTTP request details if available (for debugging/auditing)
        if let Some(http_parts) = context.extensions.get::<axum::http::request::Parts>() {
            tracing::info!(
                headers = ?http_parts.headers,
                uri = %http_parts.uri,
                "MCP initialize from HTTP client"
            );
        }
        Ok(self.get_info())
    }

    async fn on_cancelled(
        &self,
        notification: CancelledNotificationParam,
        _context: NotificationContext<RoleServer>,
    ) {
        // Log cancellation request for debugging/auditing
        // Note: Full job cancellation requires request_id to job_id mapping,
        // which is a future enhancement
        tracing::info!(
            request_id = %notification.request_id,
            reason = ?notification.reason,
            "MCP request cancelled"
        );

        // TODO: Implement job cancellation when request_id to job_id mapping is available
        // This would involve:
        // 1. Tracking active requests with their job IDs
        // 2. Calling job_app.cancel(job_id) when cancellation is requested
    }

    async fn on_initialized(&self, _context: NotificationContext<RoleServer>) {
        tracing::info!("MCP client initialized successfully");
    }
}

#[cfg(test)]
mod tests {
    // Integration tests are in tests/handler_integration_test.rs
}
