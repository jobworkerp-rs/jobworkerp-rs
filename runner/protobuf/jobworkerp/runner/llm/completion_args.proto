syntax = "proto3";
package jobworkerp.runner.llm;

// Completion request (text completion)
message LLMCompletionArgs {
  // LLM request options for generation configuration
  // To specify to change the default behavior of the LLM
  message LLMOptions {
    // The length of the sample to generate (in tokens) (default: context size).
    optional int32 max_tokens = 1;

    // The temperature used to generate samples.
    optional float temperature = 2;

    // Nucleus sampling probability cutoff.
    optional float top_p = 3;

    // Penalty to be applied for repeating tokens, 1. means no penalty.
    optional float repeat_penalty = 4;

    // The context size to consider for the repeat penalty.
    optional int32 repeat_last_n = 5;

    // The seed to use when generating random samples.
    optional int32 seed = 6;

    // extract reasoning content from the response
    optional bool extract_reasoning_content = 7;
  }

  message FunctionOptions {
    // use function calling
    bool use_function_calling = 1;
    // function set name
    optional string function_set_name = 2;
    // use runners as function (exclusive with function_set_name)
    optional bool use_runners_as_function = 3;
    // use workers as function (exclusive with function_set_name)
    optional bool use_workers_as_function = 4;
  }

  // An encoding of a conversation returned by Ollama after a completion
  // request, this can be sent in a new request to keep a conversational memory.
  message OllamaContext { repeated int32 data = 1; }

  message GenerationContext {
    oneof context {
      // The context returned by Ollama.
      OllamaContext ollama_context = 1;
    }
  }

  // The name of the model (override default in runner settings).
  optional string model = 1;

  // The system prompt to use for the system output.
  // (override the default system prompt)
  optional string system_prompt = 2;

  // The initial prompt (override runner settings).
  string prompt = 3;

  // The options to use for the model.
  optional LLMOptions options = 4;

  // An encoding of the conversation used in this response, this can be sent in
  // the next request to keep a conversational memory
  optional GenerationContext context = 5;

  // The function options to use for the model.
  optional FunctionOptions function_options = 6;
}
