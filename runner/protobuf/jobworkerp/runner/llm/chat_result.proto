syntax = "proto3";
package jobworkerp.runner.llm;

// Container for various LLM response formats
message LLMChatResult {

  // Usage statistics for a request
  message Usage {
    // The name of the model used for the completion.
    string model = 1;
    // The number of tokens used in the prompt.
    optional uint32 prompt_tokens = 2;
    // The number of tokens used in the completion.
    optional uint32 completion_tokens = 3;
    // The total seconds taken to process the request prompt.
    optional float total_prompt_time_sec = 4;
    // The total seconds taken to process the completion.
    optional float total_completion_time_sec = 5;
  }

  message MessageContent {
    message ImageSource {
      // For models/services that support URL as input
      string url = 1;

      // The base64 string of the image
      string base64 = 2;
    }

    message Image {
      string content_type = 1;
      ImageSource source = 2;
    }

    message ToolCall {
      string call_id = 1;
      string fn_name = 2;
      string fn_arguments = 3;
    }

    // Tool calls
    message ToolCalls { repeated ToolCall calls = 1; }

    oneof content {
      // The role of the message.
      string text = 1;

      // The content of the message.
      Image image = 2;

      // The content of the message.
      ToolCalls tool_calls = 3;
    }
  }

  // The response of the completion. This can be the entire completion or only
  // a token if the completion is streaming.
  optional MessageContent content = 1;

  // The response of the reasoning content(think tag etc). This can be the
  // entire completion or only
  optional string reasoning_content = 2;

  // Whether the completion is done. If the completion is streaming, this will
  // be false until the last response.
  bool done = 3;

  // information about the inferencing process
  optional Usage usage = 4;
}
