syntax = "proto3";
package jobworkerp.runner.llm;

// settings for model initialization
message LLMRunnerSettings {
  // ollama model specific settings
  message OllamaRunnerSettings {
    // base_url (default: http://localhost:11434)
    optional string base_url = 1;
    // model name for runner default
    string model = 2;
    // default system prompt before the user prompt
    optional string system_prompt = 3;
    // pull the model from the server (set false if you want to use manually
    // loaded model)
    optional bool pull_model = 4;
  }

  // ollama model specific settings
  // NEED to SET env for auth token of the model
  // (OPENAI_API_KEY, ANTHROPIC_API_KEY, COHERE_API_KEY, GEMINI_API_KEY,
  // GROQ_API_KEY, XAI_API_KEY, DEEPSEEK_API_KEY)
  message GenaiRunnerSettings {
    // # model name
    // use api depending on the model name (of openai, anthropic, etc.)
    string model = 1;
    // default system prompt before the user prompt
    optional string system_prompt = 2;
    // # base url
    // base url to use for the api (optional, not set usually)
    optional string base_url = 3;
  }

  // runner settings
  oneof settings {
    OllamaRunnerSettings ollama = 1;
    GenaiRunnerSettings genai = 2;
    // local = 3 was removed (now provided via plugin_runner_mistral)
  }
}